"""
æ”¶é›†æŸ¥è©¢æŒ‡æ¨™
"""
import json
import re
from pathlib import Path
from datetime import datetime

def parse_query_log():
    log_file = Path("logs/query_log.txt")
    if not log_file.exists():
        print("âŒ æŸ¥è©¢æ—¥èªŒä¸å­˜åœ¨")
        return None
    content = log_file.read_text()
    query_times = []
    for line in content.split('\n'):
        match = re.search(r'(\d+\.\d+)s', line)
        if match:
            query_times.append(float(match.group(1)))
    if not query_times:
        return None
    metrics = {
        "timestamp": datetime.now().isoformat(),
        "total_queries": len(query_times),
        "avg_query_time": sum(query_times) / len(query_times),
        "min_query_time": min(query_times),
        "max_query_time": max(query_times),
        "all_query_times": query_times
    }
    return metrics

def main():
    metrics = parse_query_log()
    if metrics:
        output_path = Path("logs/query_metrics.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2)
        print("\nğŸ“Š æŸ¥è©¢æŒ‡æ¨™:")
        print(f"  ç¸½æŸ¥è©¢æ•¸: {metrics['total_queries']}")
        print(f"  å¹³å‡æ™‚é–“: {metrics['avg_query_time']:.2f} ç§’")
        print(f"  æœ€å¿«: {metrics['min_query_time']:.2f} ç§’")
        print(f"  æœ€æ…¢: {metrics['max_query_time']:.2f} ç§’")
        print(f"\nâœ… æŒ‡æ¨™å·²å„²å­˜è‡³: {output_path}")
        if metrics['avg_query_time'] < 2.0:
            print("\nâœ… æŸ¥è©¢å»¶é²ç¬¦åˆç›®æ¨™ (< 2ç§’)")
        else:
            print(f"\nâš ï¸  æŸ¥è©¢å»¶é²è¶…éç›®æ¨™: {metrics['avg_query_time']:.2f}s > 2.0s")
    else:
        print("âŒ ç„¡æ³•è§£ææŸ¥è©¢æ—¥èªŒ")

if __name__ == "__main__":
    main()
